"""
Ultra-Challenging Geometric Regularization Experiment
Creating a task that's actually difficult enough to show geometric regularization benefits

The previous experiments still showed 90% accuracy - we need a truly challenging task.
This experiment creates:
1. Extremely challenging tasks (targeting 40-60% baseline accuracy)
2. Very small models (insufficient capacity)
3. Highly noisy data (realistic conditions)
4. Limited training (see improvement before convergence)
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import warnings
import time
import json
from datetime import datetime
warnings.filterwarnings('ignore')

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from geometric_tools.immediate_improvements import GeometricRegularizationLoss

def create_ultra_challenging_dataset():
    """
    Create an ultra-challenging dataset that will actually show geometric regularization benefits
    """
    print("üìö Creating Ultra-Challenging Dataset...")
    
    # Create a task that's genuinely difficult
    texts = []
    labels = []
    
    # Use very subtle differences between classes
    # This is inspired by real NLP challenges like:
    # - Sarcasm detection
    # - Sentiment in context
    # - Subtle semantic differences
    
    # Class 0: Positive sentiment with subtle negative undertones
    class_0_examples = [
        "this is good but could be better",
        "nice product though expensive",
        "works well but not perfect",
        "decent quality for the price",
        "acceptable but not great",
        "fine product with minor issues",
        "good enough for basic use",
        "satisfactory with room for improvement",
        "adequate quality at this price point",
        "reasonable choice with limitations"
    ]
    
    # Class 1: Negative sentiment with subtle positive undertones  
    class_1_examples = [
        "not bad considering the price",
        "could be worse for what you pay",
        "acceptable given the limitations",
        "not terrible for basic needs",
        "decent enough for casual use",
        "fine if you don't expect much",
        "okay for the money spent",
        "reasonable for what it offers",
        "adequate despite the flaws",
        "passable quality overall"
    ]
    
    # Class 2: Neutral with mixed signals
    class_2_examples = [
        "it is what it is",
        "nothing special but functional",
        "average product for average needs",
        "standard quality nothing more",
        "typical performance expected",
        "normal results as anticipated",
        "regular product does the job",
        "common quality meets expectations",
        "usual performance nothing exceptional",
        "standard fare for this category"
    ]
    
    # Create dataset with high ambiguity
    all_examples = class_0_examples + class_1_examples + class_2_examples
    all_labels = [0] * len(class_0_examples) + [1] * len(class_1_examples) + [2] * len(class_2_examples)
    
    # Repeat examples with variations to create more data
    for i in range(300):  # 300 examples total
        base_example = all_examples[i % len(all_examples)]
        base_label = all_labels[i % len(all_labels)]
        
        # Add random noise words that don't change meaning
        noise_words = ["really", "quite", "rather", "somewhat", "fairly", "pretty", "very", "extremely"]
        if i % 3 == 0:
            noise_word = np.random.choice(noise_words)
            if i % 2 == 0:
                base_example = noise_word + " " + base_example
            else:
                base_example = base_example + " " + noise_word
        
        texts.append(base_example)
        labels.append(base_label)
    
    # Add significant label noise (20% mislabeled)
    for i in range(60):  # 20% of 300
        # Randomly flip labels
        if labels[i] == 0:
            labels[i] = np.random.choice([1, 2])
        elif labels[i] == 1:
            labels[i] = np.random.choice([0, 2])
        else:  # labels[i] == 2
            labels[i] = np.random.choice([0, 1])
    
    # Create vocabulary
    all_words = []
    for text in texts:
        all_words.extend(text.lower().split())
    
    unique_words = list(set(all_words))
    vocab_size = min(100, len(unique_words))  # Small vocab to force compression
    
    # Create word to id mapping
    word_to_id = {word: i for i, word in enumerate(unique_words[:vocab_size])}
    word_to_id['<PAD>'] = vocab_size
    word_to_id['<UNK>'] = vocab_size + 1
    vocab_size += 2
    
    print(f"  ‚úÖ Created {len(texts)} ultra-challenging examples")
    print(f"  ‚úÖ Vocabulary size: {vocab_size} (small for compression)")
    print(f"  ‚úÖ Label distribution: {np.bincount(labels)}")
    print(f"  ‚úÖ Added 20% label noise for realism")
    
    def tokenize(text, max_length=8):  # Short sequences
        words = text.lower().split()[:max_length]
        token_ids = [word_to_id.get(word, word_to_id['<UNK>']) for word in words]
        
        # Pad to max_length
        while len(token_ids) < max_length:
            token_ids.append(word_to_id['<PAD>'])
        
        return torch.tensor(token_ids[:max_length])
    
    input_ids = torch.stack([tokenize(text) for text in texts])
    labels_tensor = torch.tensor(labels)
    
    return input_ids, labels_tensor, vocab_size

class UltraSmallModel(nn.Module):
    """
    Ultra-small model that will definitely struggle with the challenging task
    """
    def __init__(self, vocab_size, d_model=16, num_classes=3):  # Very small
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        
        # Minimal embedding layer
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # Single layer transformer (minimal capacity)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead=2, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)
        
        # Classification head
        self.classifier = nn.Linear(d_model, num_classes)
        
    def forward(self, input_ids):
        emb = self.embedding(input_ids)
        hidden = self.transformer(emb)
        cls_output = hidden[:, 0, :]  # Use first token
        return self.classifier(cls_output)
    
    def get_embeddings(self, input_ids):
        return self.embedding(input_ids)

class ImprovedUltraSmallModel(nn.Module):
    """
    Improved ultra-small model with geometric enhancements
    """
    def __init__(self, vocab_size, d_model=16, num_classes=3):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        
        # Minimal embedding layer
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # Geometric enhancement layer
        self.geometric_layer = nn.Linear(d_model, d_model)
        
        # Single layer transformer
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead=2, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)
        
        # Classification head
        self.classifier = nn.Linear(d_model, num_classes)
        
    def forward(self, input_ids):
        emb = self.embedding(input_ids)
        
        # Apply geometric enhancement
        geo_emb = self.geometric_layer(emb)
        emb = emb + 0.2 * geo_emb  # Stronger geometric influence
        
        hidden = self.transformer(emb)
        cls_output = hidden[:, 0, :]
        return self.classifier(cls_output)
    
    def get_embeddings(self, input_ids):
        emb = self.embedding(input_ids)
        geo_emb = self.geometric_layer(emb)
        return emb + 0.2 * geo_emb

def test_ultra_challenging_scenario():
    """
    Test geometric regularization on ultra-challenging scenario
    """
    print("\nüîç Testing Ultra-Challenging Scenario...")
    
    # Create ultra-challenging dataset
    input_ids, labels, vocab_size = create_ultra_challenging_dataset()
    
    print(f"\nüìä Dataset Statistics:")
    print(f"  Input shape: {input_ids.shape}")
    print(f"  Label distribution: {torch.bincount(labels).tolist()}")
    print(f"  Vocabulary size: {vocab_size}")
    
    # Test different regularization strengths
    regularization_configs = [
        {'lambda_strata': 0.001, 'lambda_curvature': 0.001, 'lambda_manifold': 0.0005, 'name': 'Ultra-Minimal'},
        {'lambda_strata': 0.01, 'lambda_curvature': 0.01, 'lambda_manifold': 0.005, 'name': 'Light'},
        {'lambda_strata': 0.05, 'lambda_curvature': 0.05, 'lambda_manifold': 0.025, 'name': 'Medium'},
        {'lambda_strata': 0.1, 'lambda_curvature': 0.1, 'lambda_manifold': 0.05, 'name': 'Strong'}
    ]
    
    results = {}
    
    for reg_config in regularization_configs:
        reg_name = reg_config['name']
        print(f"\n  Testing {reg_name} Regularization...")
        
        # Create ultra-small models
        standard_model = UltraSmallModel(vocab_size, d_model=16, num_classes=3)
        improved_model = ImprovedUltraSmallModel(vocab_size, d_model=16, num_classes=3)
        
        # Create geometric regularization
        geometric_loss = GeometricRegularizationLoss(
            reg_config['lambda_strata'],
            reg_config['lambda_curvature'],
            reg_config['lambda_manifold']
        )
        
        print(f"    Model parameters: {sum(p.numel() for p in standard_model.parameters())}")
        
        # Training setup
        optimizer_std = torch.optim.Adam(standard_model.parameters(), lr=0.01)
        optimizer_imp = torch.optim.Adam(improved_model.parameters(), lr=0.01)
        
        # Track training progress
        std_losses = []
        imp_losses = []
        std_accs = []
        imp_accs = []
        
        # Limited training to see improvement
        for epoch in range(10):
            # Train standard model
            standard_model.train()
            optimizer_std.zero_grad()
            std_outputs = standard_model(input_ids)
            std_loss = F.cross_entropy(std_outputs, labels)
            std_loss.backward()
            optimizer_std.step()
            
            # Train improved model
            improved_model.train()
            optimizer_imp.zero_grad()
            imp_outputs = improved_model(input_ids)
            imp_embeddings = improved_model.get_embeddings(input_ids)
            imp_losses_dict = geometric_loss(imp_embeddings, imp_outputs, labels)
            imp_losses_dict['total_loss'].backward()
            optimizer_imp.step()
            
            # Track progress
            std_losses.append(std_loss.item())
            imp_losses.append(imp_losses_dict['total_loss'].item())
            
            # Evaluate accuracy
            with torch.no_grad():
                std_acc = (torch.argmax(std_outputs, dim=1) == labels).float().mean()
                imp_acc = (torch.argmax(imp_outputs, dim=1) == labels).float().mean()
                std_accs.append(std_acc.item())
                imp_accs.append(imp_acc.item())
            
            if epoch % 2 == 0:
                print(f"      Epoch {epoch}: Std Acc={std_acc.item():.4f}, Imp Acc={imp_acc.item():.4f}")
        
        # Final evaluation
        standard_model.eval()
        improved_model.eval()
        
        with torch.no_grad():
            # Standard model
            standard_outputs = standard_model(input_ids)
            standard_loss = F.cross_entropy(standard_outputs, labels)
            standard_acc = (torch.argmax(standard_outputs, dim=1) == labels).float().mean()
            
            # Improved model
            improved_outputs = improved_model(input_ids)
            improved_loss = F.cross_entropy(improved_outputs, labels)
            improved_acc = (torch.argmax(improved_outputs, dim=1) == labels).float().mean()
            
            # Geometric loss
            embeddings = improved_model.get_embeddings(input_ids)
            geo_losses = geometric_loss(embeddings, improved_outputs, labels)
        
        # Calculate improvement
        acc_improvement = (improved_acc.item() - standard_acc.item()) / standard_acc.item() * 100 if standard_acc.item() > 0 else 0
        
        results[reg_name] = {
            'standard_loss': standard_loss.item(),
            'standard_acc': standard_acc.item(),
            'improved_loss': improved_loss.item(),
            'improved_acc': improved_acc.item(),
            'acc_improvement': acc_improvement,
            'geometric_loss': geo_losses['total_geometric'].item(),
            'training_curves': {
                'std_losses': std_losses,
                'imp_losses': imp_losses,
                'std_accs': std_accs,
                'imp_accs': imp_accs
            }
        }
        
        print(f"    Final Results:")
        print(f"      Standard: Loss={standard_loss.item():.4f}, Acc={standard_acc.item():.4f}")
        print(f"      Improved: Loss={improved_loss.item():.4f}, Acc={improved_acc.item():.4f}")
        print(f"      Improvement: {acc_improvement:.1f}%")
        print(f"      Geometric Loss: {geo_losses['total_geometric'].item():.6f}")
    
    return results

def run_ultra_challenging_experiment():
    """
    Run the ultra-challenging geometric regularization experiment
    """
    print("üöÄ Starting Ultra-Challenging Geometric Regularization Experiment")
    print("=" * 70)
    print("Testing on genuinely difficult tasks where regularization should help")
    print("=" * 70)
    
    # Test ultra-challenging scenario
    print("\n1. Testing Ultra-Challenging Scenario...")
    results = test_ultra_challenging_scenario()
    
    # Summary
    print("\nüìä Experiment Summary:")
    print("=" * 50)
    
    improvements = []
    for reg_name, reg_results in results.items():
        improvement = reg_results['acc_improvement']
        print(f"{reg_name}: {improvement:.1f}% improvement")
        improvements.append(improvement)
    
    if improvements:
        max_improvement = max(improvements)
        avg_improvement = np.mean(improvements)
        positive_count = sum(1 for x in improvements if x > 0)
        
        print(f"\nüìà Overall Results:")
        print(f"  Maximum improvement: {max_improvement:.1f}%")
        print(f"  Average improvement: {avg_improvement:.1f}%")
        print(f"  Positive improvements: {positive_count}/{len(improvements)}")
        
        if max_improvement > 0:
            print(f"\n‚úÖ SUCCESS! Geometric regularization shows improvement!")
            print(f"‚úÖ The framework works on genuinely challenging tasks!")
        else:
            print(f"\n‚ùå Still no improvement - task may still be too easy")
            print(f"‚ùå May need even more challenging conditions")
    
    print("\n‚úÖ Ultra-challenging experiment complete!")
    return results

if __name__ == "__main__":
    run_ultra_challenging_experiment()
